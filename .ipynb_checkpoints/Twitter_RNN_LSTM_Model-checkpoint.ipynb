{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initial imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import tensorflow as tf\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    " # Set the random seed for reproducibility\n",
    "from numpy.random import seed\n",
    "seed(1)\n",
    "\n",
    "from tensorflow import random\n",
    "random.set_seed(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date only</th>\n",
       "      <th>hour</th>\n",
       "      <th>Close</th>\n",
       "      <th>hourly return</th>\n",
       "      <th>tick</th>\n",
       "      <th>text</th>\n",
       "      <th>Subjectivity</th>\n",
       "      <th>Polarity</th>\n",
       "      <th>Analysis</th>\n",
       "      <th>date_x</th>\n",
       "      <th>date_y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2020-10-12</td>\n",
       "      <td>18</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>good grief! spacex is getting zero money for t...</td>\n",
       "      <td>0.475000</td>\n",
       "      <td>-0.100000</td>\n",
       "      <td>Negative</td>\n",
       "      <td>2020-10-12 18:58:32-05:00</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2020-10-12</td>\n",
       "      <td>16</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>with fsd, we got stuck in a lot of local maxim...</td>\n",
       "      <td>0.306349</td>\n",
       "      <td>0.128571</td>\n",
       "      <td>Positive</td>\n",
       "      <td>2020-10-12 16:23:59-05:00</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2020-10-12</td>\n",
       "      <td>16</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>thank you, much more accurate haha</td>\n",
       "      <td>0.477778</td>\n",
       "      <td>0.366667</td>\n",
       "      <td>Positive</td>\n",
       "      <td>2020-10-12 16:20:01-05:00</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2020-10-12</td>\n",
       "      <td>16</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>this graph appears to be upside down</td>\n",
       "      <td>0.288889</td>\n",
       "      <td>-0.155556</td>\n",
       "      <td>Negative</td>\n",
       "      <td>2020-10-12 16:11:19-05:00</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2020-10-12</td>\n",
       "      <td>15</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>sheer magnitude of the entire production syste...</td>\n",
       "      <td>0.652778</td>\n",
       "      <td>0.026389</td>\n",
       "      <td>Positive</td>\n",
       "      <td>2020-10-12 15:29:44-05:00</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70</th>\n",
       "      <td>2020-10-07</td>\n",
       "      <td>10</td>\n",
       "      <td>422.802399</td>\n",
       "      <td>0.006924</td>\n",
       "      <td>1.0</td>\n",
       "      <td>prototypes are a piece of cake, but high volum...</td>\n",
       "      <td>0.539697</td>\n",
       "      <td>0.001861</td>\n",
       "      <td>Positive</td>\n",
       "      <td>2020-10-07 10:02:17-05:00</td>\n",
       "      <td>2020-10-07 10:30:00-05:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71</th>\n",
       "      <td>2020-10-07</td>\n",
       "      <td>9</td>\n",
       "      <td>419.894989</td>\n",
       "      <td>0.013142</td>\n",
       "      <td>1.0</td>\n",
       "      <td>we do expect to make heavy use of lfp for medi...</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>-0.200000</td>\n",
       "      <td>Negative</td>\n",
       "      <td>2020-10-07 09:55:28-05:00</td>\n",
       "      <td>2020-10-07 09:30:00-05:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72</th>\n",
       "      <td>2020-10-07</td>\n",
       "      <td>9</td>\n",
       "      <td>419.894989</td>\n",
       "      <td>0.013142</td>\n",
       "      <td>1.0</td>\n",
       "      <td>berlin will use 4680 cell with structural batt...</td>\n",
       "      <td>0.490584</td>\n",
       "      <td>0.142532</td>\n",
       "      <td>Positive</td>\n",
       "      <td>2020-10-07 09:53:15-05:00</td>\n",
       "      <td>2020-10-07 09:30:00-05:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73</th>\n",
       "      <td>2020-10-07</td>\n",
       "      <td>9</td>\n",
       "      <td>419.894989</td>\n",
       "      <td>0.013142</td>\n",
       "      <td>1.0</td>\n",
       "      <td>they are</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>Neutral</td>\n",
       "      <td>2020-10-07 09:37:49-05:00</td>\n",
       "      <td>2020-10-07 09:30:00-05:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74</th>\n",
       "      <td>2020-10-07</td>\n",
       "      <td>9</td>\n",
       "      <td>419.894989</td>\n",
       "      <td>0.013142</td>\n",
       "      <td>1.0</td>\n",
       "      <td>i do so wish that more companies would put dow...</td>\n",
       "      <td>0.394444</td>\n",
       "      <td>0.172222</td>\n",
       "      <td>Positive</td>\n",
       "      <td>2020-10-07 09:33:53-05:00</td>\n",
       "      <td>2020-10-07 09:30:00-05:00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>75 rows Ã— 11 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     date only  hour       Close  hourly return  tick  \\\n",
       "0   2020-10-12    18         NaN            NaN   NaN   \n",
       "1   2020-10-12    16         NaN            NaN   NaN   \n",
       "2   2020-10-12    16         NaN            NaN   NaN   \n",
       "3   2020-10-12    16         NaN            NaN   NaN   \n",
       "4   2020-10-12    15         NaN            NaN   NaN   \n",
       "..         ...   ...         ...            ...   ...   \n",
       "70  2020-10-07    10  422.802399       0.006924   1.0   \n",
       "71  2020-10-07     9  419.894989       0.013142   1.0   \n",
       "72  2020-10-07     9  419.894989       0.013142   1.0   \n",
       "73  2020-10-07     9  419.894989       0.013142   1.0   \n",
       "74  2020-10-07     9  419.894989       0.013142   1.0   \n",
       "\n",
       "                                                 text  Subjectivity  Polarity  \\\n",
       "0   good grief! spacex is getting zero money for t...      0.475000 -0.100000   \n",
       "1   with fsd, we got stuck in a lot of local maxim...      0.306349  0.128571   \n",
       "2                  thank you, much more accurate haha      0.477778  0.366667   \n",
       "3                this graph appears to be upside down      0.288889 -0.155556   \n",
       "4   sheer magnitude of the entire production syste...      0.652778  0.026389   \n",
       "..                                                ...           ...       ...   \n",
       "70  prototypes are a piece of cake, but high volum...      0.539697  0.001861   \n",
       "71  we do expect to make heavy use of lfp for medi...      0.500000 -0.200000   \n",
       "72  berlin will use 4680 cell with structural batt...      0.490584  0.142532   \n",
       "73                                           they are      0.000000  0.000000   \n",
       "74  i do so wish that more companies would put dow...      0.394444  0.172222   \n",
       "\n",
       "    Analysis                     date_x                     date_y  \n",
       "0   Negative  2020-10-12 18:58:32-05:00                        NaN  \n",
       "1   Positive  2020-10-12 16:23:59-05:00                        NaN  \n",
       "2   Positive  2020-10-12 16:20:01-05:00                        NaN  \n",
       "3   Negative  2020-10-12 16:11:19-05:00                        NaN  \n",
       "4   Positive  2020-10-12 15:29:44-05:00                        NaN  \n",
       "..       ...                        ...                        ...  \n",
       "70  Positive  2020-10-07 10:02:17-05:00  2020-10-07 10:30:00-05:00  \n",
       "71  Negative  2020-10-07 09:55:28-05:00  2020-10-07 09:30:00-05:00  \n",
       "72  Positive  2020-10-07 09:53:15-05:00  2020-10-07 09:30:00-05:00  \n",
       "73   Neutral  2020-10-07 09:37:49-05:00  2020-10-07 09:30:00-05:00  \n",
       "74  Positive  2020-10-07 09:33:53-05:00  2020-10-07 09:30:00-05:00  \n",
       "\n",
       "[75 rows x 11 columns]"
      ]
     },
     "execution_count": 168,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file_path = Path(\"C:\\\\Users\\\\Zach.000\\\\Documents\\\\SMUFinTech\\\\GitHub\\\\Portfolio\\\\project-3\\\\Resources\\\\elonmusk_tweets_hourly_price.csv\")\n",
    "df_tweet = pd.read_csv(file_path)\n",
    "df_tweet.head(75)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date only</th>\n",
       "      <th>hour</th>\n",
       "      <th>Close</th>\n",
       "      <th>hourly return</th>\n",
       "      <th>tick</th>\n",
       "      <th>text</th>\n",
       "      <th>Subjectivity</th>\n",
       "      <th>Polarity</th>\n",
       "      <th>Analysis</th>\n",
       "      <th>date_x</th>\n",
       "      <th>date_y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>2020-10-08</td>\n",
       "      <td>14</td>\n",
       "      <td>425.920013</td>\n",
       "      <td>-0.004204</td>\n",
       "      <td>0.0</td>\n",
       "      <td>meant to say that it should be fixed in the la...</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>Positive</td>\n",
       "      <td>2020-10-08 14:33:47-05:00</td>\n",
       "      <td>2020-10-08 14:30:00-05:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>2020-10-08</td>\n",
       "      <td>14</td>\n",
       "      <td>425.920013</td>\n",
       "      <td>-0.004204</td>\n",
       "      <td>0.0</td>\n",
       "      <td>it should be</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>Neutral</td>\n",
       "      <td>2020-10-08 14:32:53-05:00</td>\n",
       "      <td>2020-10-08 14:30:00-05:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>2020-10-08</td>\n",
       "      <td>14</td>\n",
       "      <td>425.920013</td>\n",
       "      <td>-0.004204</td>\n",
       "      <td>0.0</td>\n",
       "      <td>exactly. we barked up that tree for way too lo...</td>\n",
       "      <td>0.438068</td>\n",
       "      <td>-0.070455</td>\n",
       "      <td>Negative</td>\n",
       "      <td>2020-10-08 14:18:15-05:00</td>\n",
       "      <td>2020-10-08 14:30:00-05:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>2020-10-08</td>\n",
       "      <td>14</td>\n",
       "      <td>425.920013</td>\n",
       "      <td>-0.004204</td>\n",
       "      <td>0.0</td>\n",
       "      <td>yes, we will play sexy snake jazz</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>Positive</td>\n",
       "      <td>2020-10-08 14:07:41-05:00</td>\n",
       "      <td>2020-10-08 14:30:00-05:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>2020-10-08</td>\n",
       "      <td>14</td>\n",
       "      <td>425.920013</td>\n",
       "      <td>-0.004204</td>\n",
       "      <td>0.0</td>\n",
       "      <td>yeah, provided we do our metal gear snake auto...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>Neutral</td>\n",
       "      <td>2020-10-08 14:03:47-05:00</td>\n",
       "      <td>2020-10-08 14:30:00-05:00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     date only  hour       Close  hourly return  tick  \\\n",
       "41  2020-10-08    14  425.920013      -0.004204   0.0   \n",
       "42  2020-10-08    14  425.920013      -0.004204   0.0   \n",
       "43  2020-10-08    14  425.920013      -0.004204   0.0   \n",
       "44  2020-10-08    14  425.920013      -0.004204   0.0   \n",
       "45  2020-10-08    14  425.920013      -0.004204   0.0   \n",
       "\n",
       "                                                 text  Subjectivity  Polarity  \\\n",
       "41  meant to say that it should be fixed in the la...      0.500000  0.166667   \n",
       "42                                       it should be      0.000000  0.000000   \n",
       "43  exactly. we barked up that tree for way too lo...      0.438068 -0.070455   \n",
       "44                 yes, we will play sexy snake jazz       1.000000  0.500000   \n",
       "45  yeah, provided we do our metal gear snake auto...      0.000000  0.000000   \n",
       "\n",
       "    Analysis                     date_x                     date_y  \n",
       "41  Positive  2020-10-08 14:33:47-05:00  2020-10-08 14:30:00-05:00  \n",
       "42   Neutral  2020-10-08 14:32:53-05:00  2020-10-08 14:30:00-05:00  \n",
       "43  Negative  2020-10-08 14:18:15-05:00  2020-10-08 14:30:00-05:00  \n",
       "44  Positive  2020-10-08 14:07:41-05:00  2020-10-08 14:30:00-05:00  \n",
       "45   Neutral  2020-10-08 14:03:47-05:00  2020-10-08 14:30:00-05:00  "
      ]
     },
     "execution_count": 169,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_tweet.dropna(inplace=True)\n",
    "df_tweet.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(586, 11)"
      ]
     },
     "execution_count": 207,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_tweet.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    " # Create the features set (X) and the target vector (y)\n",
    "X = df_tweet[\"text\"].values\n",
    "y = df_tweet[\"tick\"].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    " # Import the Tokenizer method from Keras\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    " # Create an instance of the Tokenizer and fit it with the X text data\n",
    "tokenizer = Tokenizer(lower=True)\n",
    "tokenizer.fit_on_texts(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "word: 'the', token: 1\n",
      "word: 'is', token: 2\n",
      "word: 'to', token: 3\n",
      "word: 'a', token: 4\n",
      "word: 'of', token: 5\n"
     ]
    }
   ],
   "source": [
    " # Print the first five elements of the encoded vocabulary\n",
    "for token in list(tokenizer.word_index)[:5]:\n",
    "    print(f\"word: '{token}', token: {tokenizer.word_index[token]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform the text data to numerical sequences\n",
    "X_seq = tokenizer.texts_to_sequences(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**Text comment**\n",
      "{'meant to say that it should be fixed in the latest wide release. won need to wait for fsd.'}\n"
     ]
    }
   ],
   "source": [
    " # Contrast a sample numerical sequence with its text version\n",
    "print(\"**Text comment**\")\n",
    "print({X[0]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**Numerical sequence representation**\n",
      "[223, 3, 280, 8, 11, 22, 9, 933, 6, 1, 374, 375, 281, 125, 75, 3, 934, 10, 282]\n"
     ]
    }
   ],
   "source": [
    "print(\"**Numerical sequence representation**\")\n",
    "print(X_seq[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the pad_sequences method from Keras\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    " # Set the pad size\n",
    "max_words = 280\n",
    "\n",
    "# Pad the sequences using the pad_sequences() method\n",
    "X_pad = pad_sequences(X_seq, maxlen=max_words, padding=\"post\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    " # Create the train, test, and validation sets\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_pad, y)\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    " # Import Keras modules for model creation\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [],
   "source": [
    " # Model set-up\n",
    "vocabulary_size = len(tokenizer.word_counts.keys()) + 1\n",
    "embedding_size = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    " # Define the LSTM RNN model\n",
    "model = Sequential()\n",
    "\n",
    "# Layer 1\n",
    "model.add(Embedding(vocabulary_size, embedding_size, input_length=max_words))\n",
    "\n",
    "# Layer 2\n",
    "model.add(LSTM(units=280))\n",
    "\n",
    "# Output layer\n",
    "model.add(Dense(units=1, activation=\"sigmoid\")) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile the model\n",
    "model.compile(\n",
    "    loss=\"binary_crossentropy\",\n",
    "    optimizer=\"adam\",\n",
    "    metrics=[\n",
    "        \"accuracy\",\n",
    "        tf.keras.metrics.TruePositives(name=\"tp\"),\n",
    "        tf.keras.metrics.TrueNegatives(name=\"tn\"),\n",
    "        tf.keras.metrics.FalsePositives(name=\"fp\"),\n",
    "        tf.keras.metrics.FalseNegatives(name=\"fn\"),\n",
    "        tf.keras.metrics.Precision(name=\"precision\"),\n",
    "        tf.keras.metrics.Recall(name=\"recall\"),\n",
    "        tf.keras.metrics.AUC(name=\"auc\"),\n",
    "    ],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_7\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_7 (Embedding)      (None, 280, 64)           156352    \n",
      "_________________________________________________________________\n",
      "lstm_7 (LSTM)                (None, 280)               386400    \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 1)                 281       \n",
      "=================================================================\n",
      "Total params: 543,033\n",
      "Trainable params: 543,033\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    " # Show model summary\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "1/1 [==============================] - 8s 8s/step - loss: 0.6918 - accuracy: 0.5258 - tp: 173.0000 - tn: 0.0000e+00 - fp: 156.0000 - fn: 0.0000e+00 - precision: 0.5258 - recall: 1.0000 - auc: 0.5000 - val_loss: 0.6917 - val_accuracy: 0.5273 - val_tp: 58.0000 - val_tn: 0.0000e+00 - val_fp: 52.0000 - val_fn: 0.0000e+00 - val_precision: 0.5273 - val_recall: 1.0000 - val_auc: 0.5000\n",
      "Epoch 2/100\n",
      "1/1 [==============================] - 1s 619ms/step - loss: 0.6918 - accuracy: 0.5258 - tp: 173.0000 - tn: 0.0000e+00 - fp: 156.0000 - fn: 0.0000e+00 - precision: 0.5258 - recall: 1.0000 - auc: 0.5000 - val_loss: 0.6917 - val_accuracy: 0.5273 - val_tp: 58.0000 - val_tn: 0.0000e+00 - val_fp: 52.0000 - val_fn: 0.0000e+00 - val_precision: 0.5273 - val_recall: 1.0000 - val_auc: 0.5000\n",
      "Epoch 3/100\n",
      "1/1 [==============================] - 1s 567ms/step - loss: 0.6918 - accuracy: 0.5258 - tp: 173.0000 - tn: 0.0000e+00 - fp: 156.0000 - fn: 0.0000e+00 - precision: 0.5258 - recall: 1.0000 - auc: 0.5000 - val_loss: 0.6917 - val_accuracy: 0.5273 - val_tp: 58.0000 - val_tn: 0.0000e+00 - val_fp: 52.0000 - val_fn: 0.0000e+00 - val_precision: 0.5273 - val_recall: 1.0000 - val_auc: 0.5000\n",
      "Epoch 4/100\n",
      "1/1 [==============================] - 1s 584ms/step - loss: 0.6918 - accuracy: 0.5258 - tp: 173.0000 - tn: 0.0000e+00 - fp: 156.0000 - fn: 0.0000e+00 - precision: 0.5258 - recall: 1.0000 - auc: 0.5000 - val_loss: 0.6917 - val_accuracy: 0.5273 - val_tp: 58.0000 - val_tn: 0.0000e+00 - val_fp: 52.0000 - val_fn: 0.0000e+00 - val_precision: 0.5273 - val_recall: 1.0000 - val_auc: 0.5000\n",
      "Epoch 5/100\n",
      "1/1 [==============================] - 1s 546ms/step - loss: 0.6918 - accuracy: 0.5258 - tp: 173.0000 - tn: 0.0000e+00 - fp: 156.0000 - fn: 0.0000e+00 - precision: 0.5258 - recall: 1.0000 - auc: 0.5000 - val_loss: 0.6917 - val_accuracy: 0.5273 - val_tp: 58.0000 - val_tn: 0.0000e+00 - val_fp: 52.0000 - val_fn: 0.0000e+00 - val_precision: 0.5273 - val_recall: 1.0000 - val_auc: 0.5000\n",
      "Epoch 6/100\n",
      "1/1 [==============================] - 1s 533ms/step - loss: 0.6918 - accuracy: 0.5258 - tp: 173.0000 - tn: 0.0000e+00 - fp: 156.0000 - fn: 0.0000e+00 - precision: 0.5258 - recall: 1.0000 - auc: 0.5000 - val_loss: 0.6917 - val_accuracy: 0.5273 - val_tp: 58.0000 - val_tn: 0.0000e+00 - val_fp: 52.0000 - val_fn: 0.0000e+00 - val_precision: 0.5273 - val_recall: 1.0000 - val_auc: 0.5000\n",
      "Epoch 7/100\n",
      "1/1 [==============================] - 1s 601ms/step - loss: 0.6918 - accuracy: 0.5258 - tp: 173.0000 - tn: 0.0000e+00 - fp: 156.0000 - fn: 0.0000e+00 - precision: 0.5258 - recall: 1.0000 - auc: 0.5000 - val_loss: 0.6917 - val_accuracy: 0.5273 - val_tp: 58.0000 - val_tn: 0.0000e+00 - val_fp: 52.0000 - val_fn: 0.0000e+00 - val_precision: 0.5273 - val_recall: 1.0000 - val_auc: 0.5000\n",
      "Epoch 8/100\n",
      "1/1 [==============================] - 1s 597ms/step - loss: 0.6918 - accuracy: 0.5258 - tp: 173.0000 - tn: 0.0000e+00 - fp: 156.0000 - fn: 0.0000e+00 - precision: 0.5258 - recall: 1.0000 - auc: 0.5000 - val_loss: 0.6917 - val_accuracy: 0.5273 - val_tp: 58.0000 - val_tn: 0.0000e+00 - val_fp: 52.0000 - val_fn: 0.0000e+00 - val_precision: 0.5273 - val_recall: 1.0000 - val_auc: 0.5000\n",
      "Epoch 9/100\n",
      "1/1 [==============================] - 1s 628ms/step - loss: 0.6918 - accuracy: 0.5258 - tp: 173.0000 - tn: 0.0000e+00 - fp: 156.0000 - fn: 0.0000e+00 - precision: 0.5258 - recall: 1.0000 - auc: 0.5000 - val_loss: 0.6917 - val_accuracy: 0.5273 - val_tp: 58.0000 - val_tn: 0.0000e+00 - val_fp: 52.0000 - val_fn: 0.0000e+00 - val_precision: 0.5273 - val_recall: 1.0000 - val_auc: 0.5000\n",
      "Epoch 10/100\n",
      "1/1 [==============================] - 1s 567ms/step - loss: 0.6918 - accuracy: 0.5258 - tp: 173.0000 - tn: 0.0000e+00 - fp: 156.0000 - fn: 0.0000e+00 - precision: 0.5258 - recall: 1.0000 - auc: 0.5000 - val_loss: 0.6917 - val_accuracy: 0.5273 - val_tp: 58.0000 - val_tn: 0.0000e+00 - val_fp: 52.0000 - val_fn: 0.0000e+00 - val_precision: 0.5273 - val_recall: 1.0000 - val_auc: 0.5000\n",
      "Epoch 11/100\n",
      "1/1 [==============================] - 1s 663ms/step - loss: 0.6918 - accuracy: 0.5258 - tp: 173.0000 - tn: 0.0000e+00 - fp: 156.0000 - fn: 0.0000e+00 - precision: 0.5258 - recall: 1.0000 - auc: 0.5000 - val_loss: 0.6917 - val_accuracy: 0.5273 - val_tp: 58.0000 - val_tn: 0.0000e+00 - val_fp: 52.0000 - val_fn: 0.0000e+00 - val_precision: 0.5273 - val_recall: 1.0000 - val_auc: 0.5000\n",
      "Epoch 12/100\n",
      "1/1 [==============================] - 1s 580ms/step - loss: 0.6918 - accuracy: 0.5258 - tp: 173.0000 - tn: 0.0000e+00 - fp: 156.0000 - fn: 0.0000e+00 - precision: 0.5258 - recall: 1.0000 - auc: 0.5000 - val_loss: 0.6917 - val_accuracy: 0.5273 - val_tp: 58.0000 - val_tn: 0.0000e+00 - val_fp: 52.0000 - val_fn: 0.0000e+00 - val_precision: 0.5273 - val_recall: 1.0000 - val_auc: 0.5000\n",
      "Epoch 13/100\n",
      "1/1 [==============================] - 1s 575ms/step - loss: 0.6918 - accuracy: 0.5258 - tp: 173.0000 - tn: 0.0000e+00 - fp: 156.0000 - fn: 0.0000e+00 - precision: 0.5258 - recall: 1.0000 - auc: 0.5000 - val_loss: 0.6917 - val_accuracy: 0.5273 - val_tp: 58.0000 - val_tn: 0.0000e+00 - val_fp: 52.0000 - val_fn: 0.0000e+00 - val_precision: 0.5273 - val_recall: 1.0000 - val_auc: 0.5000\n",
      "Epoch 14/100\n",
      "1/1 [==============================] - 1s 555ms/step - loss: 0.6918 - accuracy: 0.5258 - tp: 173.0000 - tn: 0.0000e+00 - fp: 156.0000 - fn: 0.0000e+00 - precision: 0.5258 - recall: 1.0000 - auc: 0.5000 - val_loss: 0.6917 - val_accuracy: 0.5273 - val_tp: 58.0000 - val_tn: 0.0000e+00 - val_fp: 52.0000 - val_fn: 0.0000e+00 - val_precision: 0.5273 - val_recall: 1.0000 - val_auc: 0.5000\n",
      "Epoch 15/100\n",
      "1/1 [==============================] - 1s 581ms/step - loss: 0.6918 - accuracy: 0.5258 - tp: 173.0000 - tn: 0.0000e+00 - fp: 156.0000 - fn: 0.0000e+00 - precision: 0.5258 - recall: 1.0000 - auc: 0.5000 - val_loss: 0.6917 - val_accuracy: 0.5273 - val_tp: 58.0000 - val_tn: 0.0000e+00 - val_fp: 52.0000 - val_fn: 0.0000e+00 - val_precision: 0.5273 - val_recall: 1.0000 - val_auc: 0.5000\n",
      "Epoch 16/100\n",
      "1/1 [==============================] - 1s 580ms/step - loss: 0.6918 - accuracy: 0.5258 - tp: 173.0000 - tn: 0.0000e+00 - fp: 156.0000 - fn: 0.0000e+00 - precision: 0.5258 - recall: 1.0000 - auc: 0.5000 - val_loss: 0.6917 - val_accuracy: 0.5273 - val_tp: 58.0000 - val_tn: 0.0000e+00 - val_fp: 52.0000 - val_fn: 0.0000e+00 - val_precision: 0.5273 - val_recall: 1.0000 - val_auc: 0.5000\n",
      "Epoch 17/100\n",
      "1/1 [==============================] - 1s 572ms/step - loss: 0.6918 - accuracy: 0.5258 - tp: 173.0000 - tn: 0.0000e+00 - fp: 156.0000 - fn: 0.0000e+00 - precision: 0.5258 - recall: 1.0000 - auc: 0.5000 - val_loss: 0.6917 - val_accuracy: 0.5273 - val_tp: 58.0000 - val_tn: 0.0000e+00 - val_fp: 52.0000 - val_fn: 0.0000e+00 - val_precision: 0.5273 - val_recall: 1.0000 - val_auc: 0.5000\n",
      "Epoch 18/100\n",
      "1/1 [==============================] - 1s 568ms/step - loss: 0.6918 - accuracy: 0.5258 - tp: 173.0000 - tn: 0.0000e+00 - fp: 156.0000 - fn: 0.0000e+00 - precision: 0.5258 - recall: 1.0000 - auc: 0.5000 - val_loss: 0.6917 - val_accuracy: 0.5273 - val_tp: 58.0000 - val_tn: 0.0000e+00 - val_fp: 52.0000 - val_fn: 0.0000e+00 - val_precision: 0.5273 - val_recall: 1.0000 - val_auc: 0.5000\n",
      "Epoch 19/100\n",
      "1/1 [==============================] - 1s 618ms/step - loss: 0.6918 - accuracy: 0.5258 - tp: 173.0000 - tn: 0.0000e+00 - fp: 156.0000 - fn: 0.0000e+00 - precision: 0.5258 - recall: 1.0000 - auc: 0.5000 - val_loss: 0.6917 - val_accuracy: 0.5273 - val_tp: 58.0000 - val_tn: 0.0000e+00 - val_fp: 52.0000 - val_fn: 0.0000e+00 - val_precision: 0.5273 - val_recall: 1.0000 - val_auc: 0.5000\n",
      "Epoch 20/100\n",
      "1/1 [==============================] - 1s 566ms/step - loss: 0.6918 - accuracy: 0.5258 - tp: 173.0000 - tn: 0.0000e+00 - fp: 156.0000 - fn: 0.0000e+00 - precision: 0.5258 - recall: 1.0000 - auc: 0.5000 - val_loss: 0.6917 - val_accuracy: 0.5273 - val_tp: 58.0000 - val_tn: 0.0000e+00 - val_fp: 52.0000 - val_fn: 0.0000e+00 - val_precision: 0.5273 - val_recall: 1.0000 - val_auc: 0.5000\n",
      "Epoch 21/100\n",
      "1/1 [==============================] - 1s 634ms/step - loss: 0.6918 - accuracy: 0.5258 - tp: 173.0000 - tn: 0.0000e+00 - fp: 156.0000 - fn: 0.0000e+00 - precision: 0.5258 - recall: 1.0000 - auc: 0.5000 - val_loss: 0.6917 - val_accuracy: 0.5273 - val_tp: 58.0000 - val_tn: 0.0000e+00 - val_fp: 52.0000 - val_fn: 0.0000e+00 - val_precision: 0.5273 - val_recall: 1.0000 - val_auc: 0.5000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 22/100\n",
      "1/1 [==============================] - 1s 613ms/step - loss: 0.6918 - accuracy: 0.5258 - tp: 173.0000 - tn: 0.0000e+00 - fp: 156.0000 - fn: 0.0000e+00 - precision: 0.5258 - recall: 1.0000 - auc: 0.5000 - val_loss: 0.6917 - val_accuracy: 0.5273 - val_tp: 58.0000 - val_tn: 0.0000e+00 - val_fp: 52.0000 - val_fn: 0.0000e+00 - val_precision: 0.5273 - val_recall: 1.0000 - val_auc: 0.5000\n",
      "Epoch 23/100\n",
      "1/1 [==============================] - 1s 593ms/step - loss: 0.6918 - accuracy: 0.5258 - tp: 173.0000 - tn: 0.0000e+00 - fp: 156.0000 - fn: 0.0000e+00 - precision: 0.5258 - recall: 1.0000 - auc: 0.5000 - val_loss: 0.6917 - val_accuracy: 0.5273 - val_tp: 58.0000 - val_tn: 0.0000e+00 - val_fp: 52.0000 - val_fn: 0.0000e+00 - val_precision: 0.5273 - val_recall: 1.0000 - val_auc: 0.5000\n",
      "Epoch 24/100\n",
      "1/1 [==============================] - 1s 623ms/step - loss: 0.6918 - accuracy: 0.5258 - tp: 173.0000 - tn: 0.0000e+00 - fp: 156.0000 - fn: 0.0000e+00 - precision: 0.5258 - recall: 1.0000 - auc: 0.5000 - val_loss: 0.6917 - val_accuracy: 0.5273 - val_tp: 58.0000 - val_tn: 0.0000e+00 - val_fp: 52.0000 - val_fn: 0.0000e+00 - val_precision: 0.5273 - val_recall: 1.0000 - val_auc: 0.5000\n",
      "Epoch 25/100\n",
      "1/1 [==============================] - 1s 598ms/step - loss: 0.6918 - accuracy: 0.5258 - tp: 173.0000 - tn: 0.0000e+00 - fp: 156.0000 - fn: 0.0000e+00 - precision: 0.5258 - recall: 1.0000 - auc: 0.5000 - val_loss: 0.6917 - val_accuracy: 0.5273 - val_tp: 58.0000 - val_tn: 0.0000e+00 - val_fp: 52.0000 - val_fn: 0.0000e+00 - val_precision: 0.5273 - val_recall: 1.0000 - val_auc: 0.5000\n",
      "Epoch 26/100\n",
      "1/1 [==============================] - 1s 613ms/step - loss: 0.6918 - accuracy: 0.5258 - tp: 173.0000 - tn: 0.0000e+00 - fp: 156.0000 - fn: 0.0000e+00 - precision: 0.5258 - recall: 1.0000 - auc: 0.5000 - val_loss: 0.6917 - val_accuracy: 0.5273 - val_tp: 58.0000 - val_tn: 0.0000e+00 - val_fp: 52.0000 - val_fn: 0.0000e+00 - val_precision: 0.5273 - val_recall: 1.0000 - val_auc: 0.5000\n",
      "Epoch 27/100\n",
      "1/1 [==============================] - 1s 561ms/step - loss: 0.6918 - accuracy: 0.5258 - tp: 173.0000 - tn: 0.0000e+00 - fp: 156.0000 - fn: 0.0000e+00 - precision: 0.5258 - recall: 1.0000 - auc: 0.5000 - val_loss: 0.6917 - val_accuracy: 0.5273 - val_tp: 58.0000 - val_tn: 0.0000e+00 - val_fp: 52.0000 - val_fn: 0.0000e+00 - val_precision: 0.5273 - val_recall: 1.0000 - val_auc: 0.5000\n",
      "Epoch 28/100\n",
      "1/1 [==============================] - 1s 611ms/step - loss: 0.6918 - accuracy: 0.5258 - tp: 173.0000 - tn: 0.0000e+00 - fp: 156.0000 - fn: 0.0000e+00 - precision: 0.5258 - recall: 1.0000 - auc: 0.5000 - val_loss: 0.6917 - val_accuracy: 0.5273 - val_tp: 58.0000 - val_tn: 0.0000e+00 - val_fp: 52.0000 - val_fn: 0.0000e+00 - val_precision: 0.5273 - val_recall: 1.0000 - val_auc: 0.5000\n",
      "Epoch 29/100\n",
      "1/1 [==============================] - 1s 562ms/step - loss: 0.6918 - accuracy: 0.5258 - tp: 173.0000 - tn: 0.0000e+00 - fp: 156.0000 - fn: 0.0000e+00 - precision: 0.5258 - recall: 1.0000 - auc: 0.5000 - val_loss: 0.6917 - val_accuracy: 0.5273 - val_tp: 58.0000 - val_tn: 0.0000e+00 - val_fp: 52.0000 - val_fn: 0.0000e+00 - val_precision: 0.5273 - val_recall: 1.0000 - val_auc: 0.5000\n",
      "Epoch 30/100\n",
      "1/1 [==============================] - 1s 570ms/step - loss: 0.6918 - accuracy: 0.5258 - tp: 173.0000 - tn: 0.0000e+00 - fp: 156.0000 - fn: 0.0000e+00 - precision: 0.5258 - recall: 1.0000 - auc: 0.5000 - val_loss: 0.6917 - val_accuracy: 0.5273 - val_tp: 58.0000 - val_tn: 0.0000e+00 - val_fp: 52.0000 - val_fn: 0.0000e+00 - val_precision: 0.5273 - val_recall: 1.0000 - val_auc: 0.5000\n",
      "Epoch 31/100\n",
      "1/1 [==============================] - 1s 557ms/step - loss: 0.6918 - accuracy: 0.5258 - tp: 173.0000 - tn: 0.0000e+00 - fp: 156.0000 - fn: 0.0000e+00 - precision: 0.5258 - recall: 1.0000 - auc: 0.5000 - val_loss: 0.6917 - val_accuracy: 0.5273 - val_tp: 58.0000 - val_tn: 0.0000e+00 - val_fp: 52.0000 - val_fn: 0.0000e+00 - val_precision: 0.5273 - val_recall: 1.0000 - val_auc: 0.5000\n",
      "Epoch 32/100\n",
      "1/1 [==============================] - 1s 629ms/step - loss: 0.6918 - accuracy: 0.5258 - tp: 173.0000 - tn: 0.0000e+00 - fp: 156.0000 - fn: 0.0000e+00 - precision: 0.5258 - recall: 1.0000 - auc: 0.5000 - val_loss: 0.6917 - val_accuracy: 0.5273 - val_tp: 58.0000 - val_tn: 0.0000e+00 - val_fp: 52.0000 - val_fn: 0.0000e+00 - val_precision: 0.5273 - val_recall: 1.0000 - val_auc: 0.5000\n",
      "Epoch 33/100\n",
      "1/1 [==============================] - 1s 595ms/step - loss: 0.6918 - accuracy: 0.5258 - tp: 173.0000 - tn: 0.0000e+00 - fp: 156.0000 - fn: 0.0000e+00 - precision: 0.5258 - recall: 1.0000 - auc: 0.5000 - val_loss: 0.6917 - val_accuracy: 0.5273 - val_tp: 58.0000 - val_tn: 0.0000e+00 - val_fp: 52.0000 - val_fn: 0.0000e+00 - val_precision: 0.5273 - val_recall: 1.0000 - val_auc: 0.5000\n",
      "Epoch 34/100\n",
      "1/1 [==============================] - 1s 589ms/step - loss: 0.6918 - accuracy: 0.5258 - tp: 173.0000 - tn: 0.0000e+00 - fp: 156.0000 - fn: 0.0000e+00 - precision: 0.5258 - recall: 1.0000 - auc: 0.5000 - val_loss: 0.6917 - val_accuracy: 0.5273 - val_tp: 58.0000 - val_tn: 0.0000e+00 - val_fp: 52.0000 - val_fn: 0.0000e+00 - val_precision: 0.5273 - val_recall: 1.0000 - val_auc: 0.5000\n",
      "Epoch 35/100\n",
      "1/1 [==============================] - 1s 560ms/step - loss: 0.6918 - accuracy: 0.5258 - tp: 173.0000 - tn: 0.0000e+00 - fp: 156.0000 - fn: 0.0000e+00 - precision: 0.5258 - recall: 1.0000 - auc: 0.5000 - val_loss: 0.6917 - val_accuracy: 0.5273 - val_tp: 58.0000 - val_tn: 0.0000e+00 - val_fp: 52.0000 - val_fn: 0.0000e+00 - val_precision: 0.5273 - val_recall: 1.0000 - val_auc: 0.5000\n",
      "Epoch 36/100\n",
      "1/1 [==============================] - 1s 561ms/step - loss: 0.6918 - accuracy: 0.5258 - tp: 173.0000 - tn: 0.0000e+00 - fp: 156.0000 - fn: 0.0000e+00 - precision: 0.5258 - recall: 1.0000 - auc: 0.5000 - val_loss: 0.6917 - val_accuracy: 0.5273 - val_tp: 58.0000 - val_tn: 0.0000e+00 - val_fp: 52.0000 - val_fn: 0.0000e+00 - val_precision: 0.5273 - val_recall: 1.0000 - val_auc: 0.5000\n",
      "Epoch 37/100\n",
      "1/1 [==============================] - 1s 559ms/step - loss: 0.6918 - accuracy: 0.5258 - tp: 173.0000 - tn: 0.0000e+00 - fp: 156.0000 - fn: 0.0000e+00 - precision: 0.5258 - recall: 1.0000 - auc: 0.5000 - val_loss: 0.6917 - val_accuracy: 0.5273 - val_tp: 58.0000 - val_tn: 0.0000e+00 - val_fp: 52.0000 - val_fn: 0.0000e+00 - val_precision: 0.5273 - val_recall: 1.0000 - val_auc: 0.5000\n",
      "Epoch 38/100\n",
      "1/1 [==============================] - 1s 546ms/step - loss: 0.6918 - accuracy: 0.5258 - tp: 173.0000 - tn: 0.0000e+00 - fp: 156.0000 - fn: 0.0000e+00 - precision: 0.5258 - recall: 1.0000 - auc: 0.5000 - val_loss: 0.6917 - val_accuracy: 0.5273 - val_tp: 58.0000 - val_tn: 0.0000e+00 - val_fp: 52.0000 - val_fn: 0.0000e+00 - val_precision: 0.5273 - val_recall: 1.0000 - val_auc: 0.5000\n",
      "Epoch 39/100\n",
      "1/1 [==============================] - 1s 584ms/step - loss: 0.6918 - accuracy: 0.5258 - tp: 173.0000 - tn: 0.0000e+00 - fp: 156.0000 - fn: 0.0000e+00 - precision: 0.5258 - recall: 1.0000 - auc: 0.5000 - val_loss: 0.6917 - val_accuracy: 0.5273 - val_tp: 58.0000 - val_tn: 0.0000e+00 - val_fp: 52.0000 - val_fn: 0.0000e+00 - val_precision: 0.5273 - val_recall: 1.0000 - val_auc: 0.5000\n",
      "Epoch 40/100\n",
      "1/1 [==============================] - 1s 572ms/step - loss: 0.6918 - accuracy: 0.5258 - tp: 173.0000 - tn: 0.0000e+00 - fp: 156.0000 - fn: 0.0000e+00 - precision: 0.5258 - recall: 1.0000 - auc: 0.5000 - val_loss: 0.6917 - val_accuracy: 0.5273 - val_tp: 58.0000 - val_tn: 0.0000e+00 - val_fp: 52.0000 - val_fn: 0.0000e+00 - val_precision: 0.5273 - val_recall: 1.0000 - val_auc: 0.5000\n",
      "Epoch 41/100\n",
      "1/1 [==============================] - 1s 565ms/step - loss: 0.6918 - accuracy: 0.5258 - tp: 173.0000 - tn: 0.0000e+00 - fp: 156.0000 - fn: 0.0000e+00 - precision: 0.5258 - recall: 1.0000 - auc: 0.5000 - val_loss: 0.6917 - val_accuracy: 0.5273 - val_tp: 58.0000 - val_tn: 0.0000e+00 - val_fp: 52.0000 - val_fn: 0.0000e+00 - val_precision: 0.5273 - val_recall: 1.0000 - val_auc: 0.5000\n",
      "Epoch 42/100\n",
      "1/1 [==============================] - 1s 606ms/step - loss: 0.6918 - accuracy: 0.5258 - tp: 173.0000 - tn: 0.0000e+00 - fp: 156.0000 - fn: 0.0000e+00 - precision: 0.5258 - recall: 1.0000 - auc: 0.5000 - val_loss: 0.6917 - val_accuracy: 0.5273 - val_tp: 58.0000 - val_tn: 0.0000e+00 - val_fp: 52.0000 - val_fn: 0.0000e+00 - val_precision: 0.5273 - val_recall: 1.0000 - val_auc: 0.5000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 43/100\n",
      "1/1 [==============================] - 1s 595ms/step - loss: 0.6918 - accuracy: 0.5258 - tp: 173.0000 - tn: 0.0000e+00 - fp: 156.0000 - fn: 0.0000e+00 - precision: 0.5258 - recall: 1.0000 - auc: 0.5000 - val_loss: 0.6917 - val_accuracy: 0.5273 - val_tp: 58.0000 - val_tn: 0.0000e+00 - val_fp: 52.0000 - val_fn: 0.0000e+00 - val_precision: 0.5273 - val_recall: 1.0000 - val_auc: 0.5000\n",
      "Epoch 44/100\n",
      "1/1 [==============================] - 1s 621ms/step - loss: 0.6918 - accuracy: 0.5258 - tp: 173.0000 - tn: 0.0000e+00 - fp: 156.0000 - fn: 0.0000e+00 - precision: 0.5258 - recall: 1.0000 - auc: 0.5000 - val_loss: 0.6917 - val_accuracy: 0.5273 - val_tp: 58.0000 - val_tn: 0.0000e+00 - val_fp: 52.0000 - val_fn: 0.0000e+00 - val_precision: 0.5273 - val_recall: 1.0000 - val_auc: 0.5000\n",
      "Epoch 45/100\n",
      "1/1 [==============================] - 1s 590ms/step - loss: 0.6918 - accuracy: 0.5258 - tp: 173.0000 - tn: 0.0000e+00 - fp: 156.0000 - fn: 0.0000e+00 - precision: 0.5258 - recall: 1.0000 - auc: 0.5000 - val_loss: 0.6917 - val_accuracy: 0.5273 - val_tp: 58.0000 - val_tn: 0.0000e+00 - val_fp: 52.0000 - val_fn: 0.0000e+00 - val_precision: 0.5273 - val_recall: 1.0000 - val_auc: 0.5000\n",
      "Epoch 46/100\n",
      "1/1 [==============================] - 1s 564ms/step - loss: 0.6918 - accuracy: 0.5258 - tp: 173.0000 - tn: 0.0000e+00 - fp: 156.0000 - fn: 0.0000e+00 - precision: 0.5258 - recall: 1.0000 - auc: 0.5000 - val_loss: 0.6917 - val_accuracy: 0.5273 - val_tp: 58.0000 - val_tn: 0.0000e+00 - val_fp: 52.0000 - val_fn: 0.0000e+00 - val_precision: 0.5273 - val_recall: 1.0000 - val_auc: 0.5000\n",
      "Epoch 47/100\n",
      "1/1 [==============================] - 1s 646ms/step - loss: 0.6918 - accuracy: 0.5258 - tp: 173.0000 - tn: 0.0000e+00 - fp: 156.0000 - fn: 0.0000e+00 - precision: 0.5258 - recall: 1.0000 - auc: 0.5000 - val_loss: 0.6917 - val_accuracy: 0.5273 - val_tp: 58.0000 - val_tn: 0.0000e+00 - val_fp: 52.0000 - val_fn: 0.0000e+00 - val_precision: 0.5273 - val_recall: 1.0000 - val_auc: 0.5000\n",
      "Epoch 48/100\n",
      "1/1 [==============================] - 1s 607ms/step - loss: 0.6918 - accuracy: 0.5258 - tp: 173.0000 - tn: 0.0000e+00 - fp: 156.0000 - fn: 0.0000e+00 - precision: 0.5258 - recall: 1.0000 - auc: 0.5000 - val_loss: 0.6917 - val_accuracy: 0.5273 - val_tp: 58.0000 - val_tn: 0.0000e+00 - val_fp: 52.0000 - val_fn: 0.0000e+00 - val_precision: 0.5273 - val_recall: 1.0000 - val_auc: 0.5000\n",
      "Epoch 49/100\n",
      "1/1 [==============================] - 1s 578ms/step - loss: 0.6918 - accuracy: 0.5258 - tp: 173.0000 - tn: 0.0000e+00 - fp: 156.0000 - fn: 0.0000e+00 - precision: 0.5258 - recall: 1.0000 - auc: 0.5000 - val_loss: 0.6917 - val_accuracy: 0.5273 - val_tp: 58.0000 - val_tn: 0.0000e+00 - val_fp: 52.0000 - val_fn: 0.0000e+00 - val_precision: 0.5273 - val_recall: 1.0000 - val_auc: 0.5000\n",
      "Epoch 50/100\n",
      "1/1 [==============================] - 1s 793ms/step - loss: 0.6918 - accuracy: 0.5258 - tp: 173.0000 - tn: 0.0000e+00 - fp: 156.0000 - fn: 0.0000e+00 - precision: 0.5258 - recall: 1.0000 - auc: 0.5000 - val_loss: 0.6917 - val_accuracy: 0.5273 - val_tp: 58.0000 - val_tn: 0.0000e+00 - val_fp: 52.0000 - val_fn: 0.0000e+00 - val_precision: 0.5273 - val_recall: 1.0000 - val_auc: 0.5000\n",
      "Epoch 51/100\n",
      "1/1 [==============================] - 1s 562ms/step - loss: 0.6918 - accuracy: 0.5258 - tp: 173.0000 - tn: 0.0000e+00 - fp: 156.0000 - fn: 0.0000e+00 - precision: 0.5258 - recall: 1.0000 - auc: 0.5000 - val_loss: 0.6917 - val_accuracy: 0.5273 - val_tp: 58.0000 - val_tn: 0.0000e+00 - val_fp: 52.0000 - val_fn: 0.0000e+00 - val_precision: 0.5273 - val_recall: 1.0000 - val_auc: 0.5000\n",
      "Epoch 52/100\n",
      "1/1 [==============================] - 1s 595ms/step - loss: 0.6918 - accuracy: 0.5258 - tp: 173.0000 - tn: 0.0000e+00 - fp: 156.0000 - fn: 0.0000e+00 - precision: 0.5258 - recall: 1.0000 - auc: 0.5000 - val_loss: 0.6917 - val_accuracy: 0.5273 - val_tp: 58.0000 - val_tn: 0.0000e+00 - val_fp: 52.0000 - val_fn: 0.0000e+00 - val_precision: 0.5273 - val_recall: 1.0000 - val_auc: 0.5000\n",
      "Epoch 53/100\n",
      "1/1 [==============================] - 1s 550ms/step - loss: 0.6918 - accuracy: 0.5258 - tp: 173.0000 - tn: 0.0000e+00 - fp: 156.0000 - fn: 0.0000e+00 - precision: 0.5258 - recall: 1.0000 - auc: 0.5000 - val_loss: 0.6917 - val_accuracy: 0.5273 - val_tp: 58.0000 - val_tn: 0.0000e+00 - val_fp: 52.0000 - val_fn: 0.0000e+00 - val_precision: 0.5273 - val_recall: 1.0000 - val_auc: 0.5000\n",
      "Epoch 54/100\n",
      "1/1 [==============================] - 1s 636ms/step - loss: 0.6918 - accuracy: 0.5258 - tp: 173.0000 - tn: 0.0000e+00 - fp: 156.0000 - fn: 0.0000e+00 - precision: 0.5258 - recall: 1.0000 - auc: 0.5000 - val_loss: 0.6917 - val_accuracy: 0.5273 - val_tp: 58.0000 - val_tn: 0.0000e+00 - val_fp: 52.0000 - val_fn: 0.0000e+00 - val_precision: 0.5273 - val_recall: 1.0000 - val_auc: 0.5000\n",
      "Epoch 55/100\n",
      "1/1 [==============================] - 1s 610ms/step - loss: 0.6918 - accuracy: 0.5258 - tp: 173.0000 - tn: 0.0000e+00 - fp: 156.0000 - fn: 0.0000e+00 - precision: 0.5258 - recall: 1.0000 - auc: 0.5000 - val_loss: 0.6917 - val_accuracy: 0.5273 - val_tp: 58.0000 - val_tn: 0.0000e+00 - val_fp: 52.0000 - val_fn: 0.0000e+00 - val_precision: 0.5273 - val_recall: 1.0000 - val_auc: 0.5000\n",
      "Epoch 56/100\n",
      "1/1 [==============================] - 1s 589ms/step - loss: 0.6918 - accuracy: 0.5258 - tp: 173.0000 - tn: 0.0000e+00 - fp: 156.0000 - fn: 0.0000e+00 - precision: 0.5258 - recall: 1.0000 - auc: 0.5000 - val_loss: 0.6917 - val_accuracy: 0.5273 - val_tp: 58.0000 - val_tn: 0.0000e+00 - val_fp: 52.0000 - val_fn: 0.0000e+00 - val_precision: 0.5273 - val_recall: 1.0000 - val_auc: 0.5000\n",
      "Epoch 57/100\n",
      "1/1 [==============================] - 1s 606ms/step - loss: 0.6918 - accuracy: 0.5258 - tp: 173.0000 - tn: 0.0000e+00 - fp: 156.0000 - fn: 0.0000e+00 - precision: 0.5258 - recall: 1.0000 - auc: 0.5000 - val_loss: 0.6917 - val_accuracy: 0.5273 - val_tp: 58.0000 - val_tn: 0.0000e+00 - val_fp: 52.0000 - val_fn: 0.0000e+00 - val_precision: 0.5273 - val_recall: 1.0000 - val_auc: 0.5000\n",
      "Epoch 58/100\n",
      "1/1 [==============================] - 1s 559ms/step - loss: 0.6918 - accuracy: 0.5258 - tp: 173.0000 - tn: 0.0000e+00 - fp: 156.0000 - fn: 0.0000e+00 - precision: 0.5258 - recall: 1.0000 - auc: 0.5000 - val_loss: 0.6917 - val_accuracy: 0.5273 - val_tp: 58.0000 - val_tn: 0.0000e+00 - val_fp: 52.0000 - val_fn: 0.0000e+00 - val_precision: 0.5273 - val_recall: 1.0000 - val_auc: 0.5000\n",
      "Epoch 59/100\n",
      "1/1 [==============================] - 1s 598ms/step - loss: 0.6918 - accuracy: 0.5258 - tp: 173.0000 - tn: 0.0000e+00 - fp: 156.0000 - fn: 0.0000e+00 - precision: 0.5258 - recall: 1.0000 - auc: 0.5000 - val_loss: 0.6917 - val_accuracy: 0.5273 - val_tp: 58.0000 - val_tn: 0.0000e+00 - val_fp: 52.0000 - val_fn: 0.0000e+00 - val_precision: 0.5273 - val_recall: 1.0000 - val_auc: 0.5000\n",
      "Epoch 60/100\n",
      "1/1 [==============================] - 1s 574ms/step - loss: 0.6918 - accuracy: 0.5258 - tp: 173.0000 - tn: 0.0000e+00 - fp: 156.0000 - fn: 0.0000e+00 - precision: 0.5258 - recall: 1.0000 - auc: 0.5000 - val_loss: 0.6917 - val_accuracy: 0.5273 - val_tp: 58.0000 - val_tn: 0.0000e+00 - val_fp: 52.0000 - val_fn: 0.0000e+00 - val_precision: 0.5273 - val_recall: 1.0000 - val_auc: 0.5000\n",
      "Epoch 61/100\n",
      "1/1 [==============================] - 1s 585ms/step - loss: 0.6918 - accuracy: 0.5258 - tp: 173.0000 - tn: 0.0000e+00 - fp: 156.0000 - fn: 0.0000e+00 - precision: 0.5258 - recall: 1.0000 - auc: 0.5000 - val_loss: 0.6917 - val_accuracy: 0.5273 - val_tp: 58.0000 - val_tn: 0.0000e+00 - val_fp: 52.0000 - val_fn: 0.0000e+00 - val_precision: 0.5273 - val_recall: 1.0000 - val_auc: 0.5000\n",
      "Epoch 62/100\n",
      "1/1 [==============================] - 1s 565ms/step - loss: 0.6918 - accuracy: 0.5258 - tp: 173.0000 - tn: 0.0000e+00 - fp: 156.0000 - fn: 0.0000e+00 - precision: 0.5258 - recall: 1.0000 - auc: 0.5000 - val_loss: 0.6917 - val_accuracy: 0.5273 - val_tp: 58.0000 - val_tn: 0.0000e+00 - val_fp: 52.0000 - val_fn: 0.0000e+00 - val_precision: 0.5273 - val_recall: 1.0000 - val_auc: 0.5000\n",
      "Epoch 63/100\n",
      "1/1 [==============================] - 1s 577ms/step - loss: 0.6918 - accuracy: 0.5258 - tp: 173.0000 - tn: 0.0000e+00 - fp: 156.0000 - fn: 0.0000e+00 - precision: 0.5258 - recall: 1.0000 - auc: 0.5000 - val_loss: 0.6917 - val_accuracy: 0.5273 - val_tp: 58.0000 - val_tn: 0.0000e+00 - val_fp: 52.0000 - val_fn: 0.0000e+00 - val_precision: 0.5273 - val_recall: 1.0000 - val_auc: 0.5000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 64/100\n",
      "1/1 [==============================] - 1s 564ms/step - loss: 0.6918 - accuracy: 0.5258 - tp: 173.0000 - tn: 0.0000e+00 - fp: 156.0000 - fn: 0.0000e+00 - precision: 0.5258 - recall: 1.0000 - auc: 0.5000 - val_loss: 0.6917 - val_accuracy: 0.5273 - val_tp: 58.0000 - val_tn: 0.0000e+00 - val_fp: 52.0000 - val_fn: 0.0000e+00 - val_precision: 0.5273 - val_recall: 1.0000 - val_auc: 0.5000\n",
      "Epoch 65/100\n",
      "1/1 [==============================] - 1s 583ms/step - loss: 0.6918 - accuracy: 0.5258 - tp: 173.0000 - tn: 0.0000e+00 - fp: 156.0000 - fn: 0.0000e+00 - precision: 0.5258 - recall: 1.0000 - auc: 0.5000 - val_loss: 0.6917 - val_accuracy: 0.5273 - val_tp: 58.0000 - val_tn: 0.0000e+00 - val_fp: 52.0000 - val_fn: 0.0000e+00 - val_precision: 0.5273 - val_recall: 1.0000 - val_auc: 0.5000\n",
      "Epoch 66/100\n",
      "1/1 [==============================] - 1s 581ms/step - loss: 0.6918 - accuracy: 0.5258 - tp: 173.0000 - tn: 0.0000e+00 - fp: 156.0000 - fn: 0.0000e+00 - precision: 0.5258 - recall: 1.0000 - auc: 0.5000 - val_loss: 0.6917 - val_accuracy: 0.5273 - val_tp: 58.0000 - val_tn: 0.0000e+00 - val_fp: 52.0000 - val_fn: 0.0000e+00 - val_precision: 0.5273 - val_recall: 1.0000 - val_auc: 0.5000\n",
      "Epoch 67/100\n",
      "1/1 [==============================] - 1s 575ms/step - loss: 0.6918 - accuracy: 0.5258 - tp: 173.0000 - tn: 0.0000e+00 - fp: 156.0000 - fn: 0.0000e+00 - precision: 0.5258 - recall: 1.0000 - auc: 0.5000 - val_loss: 0.6917 - val_accuracy: 0.5273 - val_tp: 58.0000 - val_tn: 0.0000e+00 - val_fp: 52.0000 - val_fn: 0.0000e+00 - val_precision: 0.5273 - val_recall: 1.0000 - val_auc: 0.5000\n",
      "Epoch 68/100\n",
      "1/1 [==============================] - 1s 559ms/step - loss: 0.6918 - accuracy: 0.5258 - tp: 173.0000 - tn: 0.0000e+00 - fp: 156.0000 - fn: 0.0000e+00 - precision: 0.5258 - recall: 1.0000 - auc: 0.5000 - val_loss: 0.6917 - val_accuracy: 0.5273 - val_tp: 58.0000 - val_tn: 0.0000e+00 - val_fp: 52.0000 - val_fn: 0.0000e+00 - val_precision: 0.5273 - val_recall: 1.0000 - val_auc: 0.5000\n",
      "Epoch 69/100\n",
      "1/1 [==============================] - 1s 616ms/step - loss: 0.6918 - accuracy: 0.5258 - tp: 173.0000 - tn: 0.0000e+00 - fp: 156.0000 - fn: 0.0000e+00 - precision: 0.5258 - recall: 1.0000 - auc: 0.5000 - val_loss: 0.6917 - val_accuracy: 0.5273 - val_tp: 58.0000 - val_tn: 0.0000e+00 - val_fp: 52.0000 - val_fn: 0.0000e+00 - val_precision: 0.5273 - val_recall: 1.0000 - val_auc: 0.5000\n",
      "Epoch 70/100\n",
      "1/1 [==============================] - 1s 613ms/step - loss: 0.6918 - accuracy: 0.5258 - tp: 173.0000 - tn: 0.0000e+00 - fp: 156.0000 - fn: 0.0000e+00 - precision: 0.5258 - recall: 1.0000 - auc: 0.5000 - val_loss: 0.6917 - val_accuracy: 0.5273 - val_tp: 58.0000 - val_tn: 0.0000e+00 - val_fp: 52.0000 - val_fn: 0.0000e+00 - val_precision: 0.5273 - val_recall: 1.0000 - val_auc: 0.5000\n",
      "Epoch 71/100\n",
      "1/1 [==============================] - 1s 583ms/step - loss: 0.6918 - accuracy: 0.5258 - tp: 173.0000 - tn: 0.0000e+00 - fp: 156.0000 - fn: 0.0000e+00 - precision: 0.5258 - recall: 1.0000 - auc: 0.5000 - val_loss: 0.6917 - val_accuracy: 0.5273 - val_tp: 58.0000 - val_tn: 0.0000e+00 - val_fp: 52.0000 - val_fn: 0.0000e+00 - val_precision: 0.5273 - val_recall: 1.0000 - val_auc: 0.5000\n",
      "Epoch 72/100\n",
      "1/1 [==============================] - 1s 582ms/step - loss: 0.6918 - accuracy: 0.5258 - tp: 173.0000 - tn: 0.0000e+00 - fp: 156.0000 - fn: 0.0000e+00 - precision: 0.5258 - recall: 1.0000 - auc: 0.5000 - val_loss: 0.6917 - val_accuracy: 0.5273 - val_tp: 58.0000 - val_tn: 0.0000e+00 - val_fp: 52.0000 - val_fn: 0.0000e+00 - val_precision: 0.5273 - val_recall: 1.0000 - val_auc: 0.5000\n",
      "Epoch 73/100\n",
      "1/1 [==============================] - 1s 583ms/step - loss: 0.6918 - accuracy: 0.5258 - tp: 173.0000 - tn: 0.0000e+00 - fp: 156.0000 - fn: 0.0000e+00 - precision: 0.5258 - recall: 1.0000 - auc: 0.5000 - val_loss: 0.6917 - val_accuracy: 0.5273 - val_tp: 58.0000 - val_tn: 0.0000e+00 - val_fp: 52.0000 - val_fn: 0.0000e+00 - val_precision: 0.5273 - val_recall: 1.0000 - val_auc: 0.5000\n",
      "Epoch 74/100\n",
      "1/1 [==============================] - 1s 594ms/step - loss: 0.6918 - accuracy: 0.5258 - tp: 173.0000 - tn: 0.0000e+00 - fp: 156.0000 - fn: 0.0000e+00 - precision: 0.5258 - recall: 1.0000 - auc: 0.5000 - val_loss: 0.6917 - val_accuracy: 0.5273 - val_tp: 58.0000 - val_tn: 0.0000e+00 - val_fp: 52.0000 - val_fn: 0.0000e+00 - val_precision: 0.5273 - val_recall: 1.0000 - val_auc: 0.5000\n",
      "Epoch 75/100\n",
      "1/1 [==============================] - 1s 565ms/step - loss: 0.6918 - accuracy: 0.5258 - tp: 173.0000 - tn: 0.0000e+00 - fp: 156.0000 - fn: 0.0000e+00 - precision: 0.5258 - recall: 1.0000 - auc: 0.5000 - val_loss: 0.6917 - val_accuracy: 0.5273 - val_tp: 58.0000 - val_tn: 0.0000e+00 - val_fp: 52.0000 - val_fn: 0.0000e+00 - val_precision: 0.5273 - val_recall: 1.0000 - val_auc: 0.5000\n",
      "Epoch 76/100\n",
      "1/1 [==============================] - 1s 606ms/step - loss: 0.6918 - accuracy: 0.5258 - tp: 173.0000 - tn: 0.0000e+00 - fp: 156.0000 - fn: 0.0000e+00 - precision: 0.5258 - recall: 1.0000 - auc: 0.5000 - val_loss: 0.6917 - val_accuracy: 0.5273 - val_tp: 58.0000 - val_tn: 0.0000e+00 - val_fp: 52.0000 - val_fn: 0.0000e+00 - val_precision: 0.5273 - val_recall: 1.0000 - val_auc: 0.5000\n",
      "Epoch 77/100\n",
      "1/1 [==============================] - 1s 647ms/step - loss: 0.6918 - accuracy: 0.5258 - tp: 173.0000 - tn: 0.0000e+00 - fp: 156.0000 - fn: 0.0000e+00 - precision: 0.5258 - recall: 1.0000 - auc: 0.5000 - val_loss: 0.6917 - val_accuracy: 0.5273 - val_tp: 58.0000 - val_tn: 0.0000e+00 - val_fp: 52.0000 - val_fn: 0.0000e+00 - val_precision: 0.5273 - val_recall: 1.0000 - val_auc: 0.5000\n",
      "Epoch 78/100\n",
      "1/1 [==============================] - 1s 576ms/step - loss: 0.6918 - accuracy: 0.5258 - tp: 173.0000 - tn: 0.0000e+00 - fp: 156.0000 - fn: 0.0000e+00 - precision: 0.5258 - recall: 1.0000 - auc: 0.5000 - val_loss: 0.6917 - val_accuracy: 0.5273 - val_tp: 58.0000 - val_tn: 0.0000e+00 - val_fp: 52.0000 - val_fn: 0.0000e+00 - val_precision: 0.5273 - val_recall: 1.0000 - val_auc: 0.5000\n",
      "Epoch 79/100\n",
      "1/1 [==============================] - 1s 580ms/step - loss: 0.6918 - accuracy: 0.5258 - tp: 173.0000 - tn: 0.0000e+00 - fp: 156.0000 - fn: 0.0000e+00 - precision: 0.5258 - recall: 1.0000 - auc: 0.5000 - val_loss: 0.6917 - val_accuracy: 0.5273 - val_tp: 58.0000 - val_tn: 0.0000e+00 - val_fp: 52.0000 - val_fn: 0.0000e+00 - val_precision: 0.5273 - val_recall: 1.0000 - val_auc: 0.5000\n",
      "Epoch 80/100\n",
      "1/1 [==============================] - 1s 563ms/step - loss: 0.6918 - accuracy: 0.5258 - tp: 173.0000 - tn: 0.0000e+00 - fp: 156.0000 - fn: 0.0000e+00 - precision: 0.5258 - recall: 1.0000 - auc: 0.5000 - val_loss: 0.6917 - val_accuracy: 0.5273 - val_tp: 58.0000 - val_tn: 0.0000e+00 - val_fp: 52.0000 - val_fn: 0.0000e+00 - val_precision: 0.5273 - val_recall: 1.0000 - val_auc: 0.5000\n",
      "Epoch 81/100\n",
      "1/1 [==============================] - 1s 649ms/step - loss: 0.6918 - accuracy: 0.5258 - tp: 173.0000 - tn: 0.0000e+00 - fp: 156.0000 - fn: 0.0000e+00 - precision: 0.5258 - recall: 1.0000 - auc: 0.5000 - val_loss: 0.6917 - val_accuracy: 0.5273 - val_tp: 58.0000 - val_tn: 0.0000e+00 - val_fp: 52.0000 - val_fn: 0.0000e+00 - val_precision: 0.5273 - val_recall: 1.0000 - val_auc: 0.5000\n",
      "Epoch 82/100\n",
      "1/1 [==============================] - 1s 588ms/step - loss: 0.6918 - accuracy: 0.5258 - tp: 173.0000 - tn: 0.0000e+00 - fp: 156.0000 - fn: 0.0000e+00 - precision: 0.5258 - recall: 1.0000 - auc: 0.5000 - val_loss: 0.6917 - val_accuracy: 0.5273 - val_tp: 58.0000 - val_tn: 0.0000e+00 - val_fp: 52.0000 - val_fn: 0.0000e+00 - val_precision: 0.5273 - val_recall: 1.0000 - val_auc: 0.5000\n",
      "Epoch 83/100\n",
      "1/1 [==============================] - 1s 569ms/step - loss: 0.6918 - accuracy: 0.5258 - tp: 173.0000 - tn: 0.0000e+00 - fp: 156.0000 - fn: 0.0000e+00 - precision: 0.5258 - recall: 1.0000 - auc: 0.5000 - val_loss: 0.6917 - val_accuracy: 0.5273 - val_tp: 58.0000 - val_tn: 0.0000e+00 - val_fp: 52.0000 - val_fn: 0.0000e+00 - val_precision: 0.5273 - val_recall: 1.0000 - val_auc: 0.5000\n",
      "Epoch 84/100\n",
      "1/1 [==============================] - 1s 555ms/step - loss: 0.6918 - accuracy: 0.5258 - tp: 173.0000 - tn: 0.0000e+00 - fp: 156.0000 - fn: 0.0000e+00 - precision: 0.5258 - recall: 1.0000 - auc: 0.5000 - val_loss: 0.6917 - val_accuracy: 0.5273 - val_tp: 58.0000 - val_tn: 0.0000e+00 - val_fp: 52.0000 - val_fn: 0.0000e+00 - val_precision: 0.5273 - val_recall: 1.0000 - val_auc: 0.5000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 85/100\n",
      "1/1 [==============================] - 1s 665ms/step - loss: 0.6918 - accuracy: 0.5258 - tp: 173.0000 - tn: 0.0000e+00 - fp: 156.0000 - fn: 0.0000e+00 - precision: 0.5258 - recall: 1.0000 - auc: 0.5000 - val_loss: 0.6917 - val_accuracy: 0.5273 - val_tp: 58.0000 - val_tn: 0.0000e+00 - val_fp: 52.0000 - val_fn: 0.0000e+00 - val_precision: 0.5273 - val_recall: 1.0000 - val_auc: 0.5000\n",
      "Epoch 86/100\n",
      "1/1 [==============================] - 1s 571ms/step - loss: 0.6918 - accuracy: 0.5258 - tp: 173.0000 - tn: 0.0000e+00 - fp: 156.0000 - fn: 0.0000e+00 - precision: 0.5258 - recall: 1.0000 - auc: 0.5000 - val_loss: 0.6917 - val_accuracy: 0.5273 - val_tp: 58.0000 - val_tn: 0.0000e+00 - val_fp: 52.0000 - val_fn: 0.0000e+00 - val_precision: 0.5273 - val_recall: 1.0000 - val_auc: 0.5000\n",
      "Epoch 87/100\n",
      "1/1 [==============================] - 1s 589ms/step - loss: 0.6918 - accuracy: 0.5258 - tp: 173.0000 - tn: 0.0000e+00 - fp: 156.0000 - fn: 0.0000e+00 - precision: 0.5258 - recall: 1.0000 - auc: 0.5000 - val_loss: 0.6917 - val_accuracy: 0.5273 - val_tp: 58.0000 - val_tn: 0.0000e+00 - val_fp: 52.0000 - val_fn: 0.0000e+00 - val_precision: 0.5273 - val_recall: 1.0000 - val_auc: 0.5000\n",
      "Epoch 88/100\n",
      "1/1 [==============================] - 1s 598ms/step - loss: 0.6918 - accuracy: 0.5258 - tp: 173.0000 - tn: 0.0000e+00 - fp: 156.0000 - fn: 0.0000e+00 - precision: 0.5258 - recall: 1.0000 - auc: 0.5000 - val_loss: 0.6917 - val_accuracy: 0.5273 - val_tp: 58.0000 - val_tn: 0.0000e+00 - val_fp: 52.0000 - val_fn: 0.0000e+00 - val_precision: 0.5273 - val_recall: 1.0000 - val_auc: 0.5000\n",
      "Epoch 89/100\n",
      "1/1 [==============================] - 1s 544ms/step - loss: 0.6918 - accuracy: 0.5258 - tp: 173.0000 - tn: 0.0000e+00 - fp: 156.0000 - fn: 0.0000e+00 - precision: 0.5258 - recall: 1.0000 - auc: 0.5000 - val_loss: 0.6917 - val_accuracy: 0.5273 - val_tp: 58.0000 - val_tn: 0.0000e+00 - val_fp: 52.0000 - val_fn: 0.0000e+00 - val_precision: 0.5273 - val_recall: 1.0000 - val_auc: 0.5000\n",
      "Epoch 90/100\n",
      "1/1 [==============================] - 1s 551ms/step - loss: 0.6918 - accuracy: 0.5258 - tp: 173.0000 - tn: 0.0000e+00 - fp: 156.0000 - fn: 0.0000e+00 - precision: 0.5258 - recall: 1.0000 - auc: 0.5000 - val_loss: 0.6917 - val_accuracy: 0.5273 - val_tp: 58.0000 - val_tn: 0.0000e+00 - val_fp: 52.0000 - val_fn: 0.0000e+00 - val_precision: 0.5273 - val_recall: 1.0000 - val_auc: 0.5000\n",
      "Epoch 91/100\n",
      "1/1 [==============================] - 1s 558ms/step - loss: 0.6918 - accuracy: 0.5258 - tp: 173.0000 - tn: 0.0000e+00 - fp: 156.0000 - fn: 0.0000e+00 - precision: 0.5258 - recall: 1.0000 - auc: 0.5000 - val_loss: 0.6917 - val_accuracy: 0.5273 - val_tp: 58.0000 - val_tn: 0.0000e+00 - val_fp: 52.0000 - val_fn: 0.0000e+00 - val_precision: 0.5273 - val_recall: 1.0000 - val_auc: 0.5000\n",
      "Epoch 92/100\n",
      "1/1 [==============================] - 1s 597ms/step - loss: 0.6918 - accuracy: 0.5258 - tp: 173.0000 - tn: 0.0000e+00 - fp: 156.0000 - fn: 0.0000e+00 - precision: 0.5258 - recall: 1.0000 - auc: 0.5000 - val_loss: 0.6917 - val_accuracy: 0.5273 - val_tp: 58.0000 - val_tn: 0.0000e+00 - val_fp: 52.0000 - val_fn: 0.0000e+00 - val_precision: 0.5273 - val_recall: 1.0000 - val_auc: 0.5000\n",
      "Epoch 93/100\n",
      "1/1 [==============================] - 1s 553ms/step - loss: 0.6918 - accuracy: 0.5258 - tp: 173.0000 - tn: 0.0000e+00 - fp: 156.0000 - fn: 0.0000e+00 - precision: 0.5258 - recall: 1.0000 - auc: 0.5000 - val_loss: 0.6917 - val_accuracy: 0.5273 - val_tp: 58.0000 - val_tn: 0.0000e+00 - val_fp: 52.0000 - val_fn: 0.0000e+00 - val_precision: 0.5273 - val_recall: 1.0000 - val_auc: 0.5000\n",
      "Epoch 94/100\n",
      "1/1 [==============================] - 1s 627ms/step - loss: 0.6918 - accuracy: 0.5258 - tp: 173.0000 - tn: 0.0000e+00 - fp: 156.0000 - fn: 0.0000e+00 - precision: 0.5258 - recall: 1.0000 - auc: 0.5000 - val_loss: 0.6917 - val_accuracy: 0.5273 - val_tp: 58.0000 - val_tn: 0.0000e+00 - val_fp: 52.0000 - val_fn: 0.0000e+00 - val_precision: 0.5273 - val_recall: 1.0000 - val_auc: 0.5000\n",
      "Epoch 95/100\n",
      "1/1 [==============================] - 1s 575ms/step - loss: 0.6918 - accuracy: 0.5258 - tp: 173.0000 - tn: 0.0000e+00 - fp: 156.0000 - fn: 0.0000e+00 - precision: 0.5258 - recall: 1.0000 - auc: 0.5000 - val_loss: 0.6917 - val_accuracy: 0.5273 - val_tp: 58.0000 - val_tn: 0.0000e+00 - val_fp: 52.0000 - val_fn: 0.0000e+00 - val_precision: 0.5273 - val_recall: 1.0000 - val_auc: 0.5000\n",
      "Epoch 96/100\n",
      "1/1 [==============================] - 1s 647ms/step - loss: 0.6918 - accuracy: 0.5258 - tp: 173.0000 - tn: 0.0000e+00 - fp: 156.0000 - fn: 0.0000e+00 - precision: 0.5258 - recall: 1.0000 - auc: 0.5000 - val_loss: 0.6917 - val_accuracy: 0.5273 - val_tp: 58.0000 - val_tn: 0.0000e+00 - val_fp: 52.0000 - val_fn: 0.0000e+00 - val_precision: 0.5273 - val_recall: 1.0000 - val_auc: 0.5000\n",
      "Epoch 97/100\n",
      "1/1 [==============================] - 1s 621ms/step - loss: 0.6918 - accuracy: 0.5258 - tp: 173.0000 - tn: 0.0000e+00 - fp: 156.0000 - fn: 0.0000e+00 - precision: 0.5258 - recall: 1.0000 - auc: 0.5000 - val_loss: 0.6917 - val_accuracy: 0.5273 - val_tp: 58.0000 - val_tn: 0.0000e+00 - val_fp: 52.0000 - val_fn: 0.0000e+00 - val_precision: 0.5273 - val_recall: 1.0000 - val_auc: 0.5000\n",
      "Epoch 98/100\n",
      "1/1 [==============================] - 1s 577ms/step - loss: 0.6918 - accuracy: 0.5258 - tp: 173.0000 - tn: 0.0000e+00 - fp: 156.0000 - fn: 0.0000e+00 - precision: 0.5258 - recall: 1.0000 - auc: 0.5000 - val_loss: 0.6917 - val_accuracy: 0.5273 - val_tp: 58.0000 - val_tn: 0.0000e+00 - val_fp: 52.0000 - val_fn: 0.0000e+00 - val_precision: 0.5273 - val_recall: 1.0000 - val_auc: 0.5000\n",
      "Epoch 99/100\n",
      "1/1 [==============================] - 1s 647ms/step - loss: 0.6918 - accuracy: 0.5258 - tp: 173.0000 - tn: 0.0000e+00 - fp: 156.0000 - fn: 0.0000e+00 - precision: 0.5258 - recall: 1.0000 - auc: 0.5000 - val_loss: 0.6917 - val_accuracy: 0.5273 - val_tp: 58.0000 - val_tn: 0.0000e+00 - val_fp: 52.0000 - val_fn: 0.0000e+00 - val_precision: 0.5273 - val_recall: 1.0000 - val_auc: 0.5000\n",
      "Epoch 100/100\n",
      "1/1 [==============================] - 1s 561ms/step - loss: 0.6918 - accuracy: 0.5258 - tp: 173.0000 - tn: 0.0000e+00 - fp: 156.0000 - fn: 0.0000e+00 - precision: 0.5258 - recall: 1.0000 - auc: 0.5000 - val_loss: 0.6917 - val_accuracy: 0.5273 - val_tp: 58.0000 - val_tn: 0.0000e+00 - val_fp: 52.0000 - val_fn: 0.0000e+00 - val_precision: 0.5273 - val_recall: 1.0000 - val_auc: 0.5000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x2e276e11508>"
      ]
     },
     "execution_count": 213,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " # Training the model\n",
    "batch_size = 586\n",
    "epochs = 100\n",
    "model.fit(\n",
    "    X_train,\n",
    "    y_train,\n",
    "    validation_data=(X_val, y_val),\n",
    "    epochs=epochs,\n",
    "    batch_size=batch_size,\n",
    "    verbose=1,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict classes using the testing data\n",
    "y_pred = model.predict_classes(X_test, batch_size=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RNN LSTM Accuracy 0.52\n"
     ]
    }
   ],
   "source": [
    "# Accuracy\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "print(\"RNN LSTM Accuracy %.2f\" %  (accuracy_score(y_test, y_pred)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [],
   "source": [
    " # Import the confusion_matrix method from sklearn\n",
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix from the RNN LSTM Model\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>Predicted</th>\n",
       "      <th>Positive(1)</th>\n",
       "      <th>Negative(0)</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Actual</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Positive(1)</th>\n",
       "      <td>TP=77</td>\n",
       "      <td>FN=0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Negative(0)</th>\n",
       "      <td>FP=70</td>\n",
       "      <td>TN=0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Predicted   Positive(1) Negative(0)\n",
       "Actual                             \n",
       "Positive(1)       TP=77        FN=0\n",
       "Negative(0)       FP=70        TN=0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    " # Confusion matrtix metrics from the RNN LSTM model\n",
    "tn_rnn, fp_rnn, fn_rnn, tp_rnn = confusion_matrix(y_test, y_pred).ravel()\n",
    "\n",
    "# Dataframe to display confusion matrix from the RNN LSTM model\n",
    "cm_rnn_df = pd.DataFrame(\n",
    "    {\n",
    "        \"Positive(1)\": [f\"TP={tp_rnn}\", f\"FP={fp_rnn}\"],\n",
    "        \"Negative(0)\": [f\"FN={fn_rnn}\", f\"TN={tn_rnn}\"],\n",
    "    },\n",
    "    index=[\"Positive(1)\", \"Negative(0)\"],\n",
    ")\n",
    "cm_rnn_df.index.name = \"Actual\"\n",
    "cm_rnn_df.columns.name = \"Predicted\"\n",
    "print(\"Confusion Matrix from the RNN LSTM Model\")\n",
    "display(cm_rnn_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [],
   "source": [
    " # Import the classification_report method from sklearn\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Report for the RNN LSTM Model\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.00      0.00      0.00         0\n",
      "         1.0       1.00      0.52      0.69       147\n",
      "\n",
      "    accuracy                           0.52       147\n",
      "   macro avg       0.50      0.26      0.34       147\n",
      "weighted avg       1.00      0.52      0.69       147\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Zach.000\\anaconda3\\envs\\dev1\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1221: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "# Display classification report for the RNN LSTM Model\n",
    "print(\"Classification Report for the RNN LSTM Model\")\n",
    "print(classification_report(y_pred, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [],
   "source": [
    " # Import the roc_curve and auc metrics from sklearn\n",
    "from sklearn.metrics import roc_curve, auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [],
   "source": [
    " # Making predictions to feed the roc_curve module\n",
    "test_predictions_rnn = model.predict(X_test, batch_size=100000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data for ROC Curve - RNN LSTM Model\n",
    "fpr_test_rnn, tpr_test_rnn, thresholds_test_rnn = roc_curve(y_test, test_predictions_rnn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [],
   "source": [
    " # AUC for the RNN LSTM Model\n",
    "auc_test_rnn = auc(fpr_test_rnn, tpr_test_rnn)\n",
    "auc_test_rnn = round(auc_test_rnn, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [],
   "source": [
    " # Dataframe to plot ROC Curve for the RNN LSTM model\n",
    "roc_df_test_rnn = pd.DataFrame({\"FPR Test\": fpr_test_rnn, \"TPR Test\": tpr_test_rnn,})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x2e277e9d108>"
      ]
     },
     "execution_count": 225,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    " roc_df_test_rnn.plot(\n",
    "    x=\"FPR Test\",\n",
    "    y=\"TPR Test\",\n",
    "    color=\"blue\",\n",
    "    style=\"--\",\n",
    "    xlim=([-0.05, 1.05]),\n",
    "    title=f\"Test ROC Curve (AUC={auc_test_rnn})\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
